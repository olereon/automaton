#!/usr/bin/env python3
"""
üß™ Enhanced Generation Download Algorithm - Comprehensive Test Suite
HIVE-TESTER-001: Quality Assurance Mission

Test suite design for the enhanced generation download automation with:
- Comprehensive algorithm validation
- Edge case and error scenario testing  
- Integration test framework
- Performance benchmarking
- Regression testing setup
"""

import sys
import os
import asyncio
import pytest
import unittest
import time
import json
import tempfile
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import threading

# Add paths for imports
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'src'))

from utils.generation_download_manager import (
    GenerationDownloadManager,
    GenerationDownloadConfig,
    DuplicateMode,
    GenerationMetadata,
    EnhancedFileNamer
)
from utils.gallery_navigation_fix import RobustGalleryNavigator
from utils.boundary_scroll_manager import BoundaryScrollManager
from core.engine import WebAutomationEngine, ActionType


class TestEnhancedAlgorithmSuite:
    """üéØ Core Algorithm Validation Tests"""

    @pytest.fixture
    def enhanced_config(self, temp_dir):
        """Create optimized configuration for enhanced algorithm"""
        config = GenerationDownloadConfig()
        config.downloads_folder = os.path.join(temp_dir, 'downloads')
        config.logs_folder = os.path.join(temp_dir, 'logs')
        config.duplicate_mode = DuplicateMode.SKIP
        config.use_enhanced_metadata = True
        config.use_conditional_logic = True
        config.adaptive_timeout_enabled = True
        config.scroll_optimization_enabled = True
        
        # Create directories
        os.makedirs(config.downloads_folder, exist_ok=True)
        os.makedirs(config.logs_folder, exist_ok=True)
        
        return config

    @pytest.fixture
    def mock_browser_with_optimizations(self):
        """Mock browser with optimization features enabled"""
        mock_page = Mock()
        
        # Mock adaptive timeouts
        mock_page.wait_for_load_state = AsyncMock(return_value=True)
        mock_page.wait_for_timeout = AsyncMock(return_value=True)
        
        # Mock optimized scrolling
        mock_page.evaluate = AsyncMock(return_value=True)
        mock_page.mouse = Mock()
        mock_page.mouse.wheel = AsyncMock(return_value=True)
        
        # Mock conditional popup handling
        mock_page.query_selector = AsyncMock(return_value=None)  # No popup by default
        mock_page.click = AsyncMock(return_value=True)
        
        return mock_page

    @pytest.mark.asyncio
    async def test_conditional_popup_handling(self, enhanced_config, mock_browser_with_optimizations):
        """üîç Test conditional popup handling (40-55% speed improvement)"""
        page = mock_browser_with_optimizations
        manager = GenerationDownloadManager(enhanced_config)
        
        # Test Case 1: No popup present (most common scenario)
        page.query_selector.return_value = None
        
        start_time = time.time()
        result = await manager._handle_conditional_popup(page, "button.ant-modal-close")
        end_time = time.time()
        
        assert result is False  # No popup found
        assert end_time - start_time < 1.0  # Fast execution
        page.query_selector.assert_called_once()
        page.click.assert_not_called()  # Should not attempt click
        
        # Test Case 2: Popup present (conditional handling)
        mock_popup = Mock()
        page.query_selector.return_value = mock_popup
        page.query_selector.reset_mock()
        
        result = await manager._handle_conditional_popup(page, "button.ant-modal-close")
        
        assert result is True  # Popup found and handled
        page.query_selector.assert_called_once()
        page.click.assert_called_once()

    @pytest.mark.asyncio
    async def test_adaptive_timeout_optimization(self, enhanced_config, mock_browser_with_optimizations):
        """‚ö° Test adaptive timeout optimization (15-20s saved per download)"""
        page = mock_browser_with_optimizations
        manager = GenerationDownloadManager(enhanced_config)
        
        # Mock successful fast response
        page.wait_for_load_state.return_value = True
        
        start_time = time.time()
        await manager._wait_with_adaptive_timeout(page, expected_state="networkidle")
        end_time = time.time()
        
        # Should complete quickly when conditions are met
        assert end_time - start_time < 2.0
        page.wait_for_load_state.assert_called()
        
        # Test timeout escalation
        page.wait_for_load_state.side_effect = asyncio.TimeoutError()
        
        with pytest.raises(asyncio.TimeoutError):
            await manager._wait_with_adaptive_timeout(page, expected_state="networkidle", max_retries=1)

    @pytest.mark.asyncio
    async def test_structure_based_prompt_extraction(self, enhanced_config):
        """üéØ Test structure-based prompt extraction (immune to UI changes)"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Mock DOM structure with relative positioning
        mock_container = Mock()
        mock_time_element = Mock()
        mock_time_element.text_content = AsyncMock(return_value="30 Aug 2025 05:15:30")
        
        mock_ellipsis = Mock()
        mock_prompt_element = Mock()
        mock_prompt_element.text_content = AsyncMock(return_value="A serene mountain landscape with snow-capped peaks and crystal clear lakes reflecting the morning sun. The scene captures the tranquility of nature with detailed textures and atmospheric lighting that creates depth and emotion in the composition.")
        
        # Test relative positioning extraction
        mock_container.query_selector_all = AsyncMock(side_effect=[
            [mock_time_element],  # Time elements
            [mock_ellipsis],      # Ellipsis elements
            [mock_prompt_element] # Prompt elements
        ])
        
        metadata = await manager._extract_metadata_with_structure_based_positioning(mock_container)
        
        assert metadata['generation_date'] == "30 Aug 2025 05:15:30"
        assert len(metadata['prompt']) == 382  # Full 382-character prompt extracted
        assert metadata['extraction_method'] == "structure_based"

    @pytest.mark.asyncio
    async def test_optimized_scroll_parameters(self, enhanced_config, mock_browser_with_optimizations):
        """üöÄ Test optimized scroll parameters (50% faster gallery navigation)"""
        page = mock_browser_with_optimizations
        navigator = RobustGalleryNavigator(page)
        
        # Mock scroll optimization
        page.evaluate.return_value = {"scrollHeight": 5000, "scrollTop": 2500}
        
        start_time = time.time()
        await navigator.scroll_with_optimized_parameters(distance=2500, speed="fast")
        end_time = time.time()
        
        # Should complete scroll operation quickly
        assert end_time - start_time < 1.0
        
        # Verify optimized scroll parameters were used
        page.evaluate.assert_called()
        assert "scrollBy" in str(page.evaluate.call_args)


class TestEdgeCasesAndErrorScenarios:
    """‚ö†Ô∏è Edge Cases and Error Scenario Testing"""

    @pytest.mark.asyncio
    async def test_network_timeout_recovery(self, enhanced_config):
        """üåê Test network timeout and recovery scenarios"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Mock page with network issues
        mock_page = Mock()
        mock_page.goto = AsyncMock(side_effect=asyncio.TimeoutError("Network timeout"))
        
        # Test retry mechanism
        with pytest.raises(asyncio.TimeoutError):
            await manager._navigate_with_retry(mock_page, "https://example.com", max_retries=2)
        
        # Verify retry attempts
        assert mock_page.goto.call_count == 2

    @pytest.mark.asyncio
    async def test_malformed_metadata_handling(self, enhanced_config):
        """üîí Test malformed/malicious metadata handling"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Test malicious script injection
        malicious_prompt = "<script>alert('XSS')</script>A normal prompt"
        sanitized = manager._sanitize_metadata(malicious_prompt)
        
        assert "<script>" not in sanitized
        assert "alert" not in sanitized
        assert "A normal prompt" in sanitized
        
        # Test SQL injection patterns
        sql_injection = "'; DROP TABLE downloads; --"
        sanitized_sql = manager._sanitize_metadata(sql_injection)
        
        assert "DROP TABLE" not in sanitized_sql
        assert "--" not in sanitized_sql

    @pytest.mark.asyncio
    async def test_memory_exhaustion_protection(self, enhanced_config):
        """üíæ Test memory exhaustion protection"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Simulate large dataset processing
        large_dataset = []
        for i in range(10000):
            large_dataset.append({
                "generation_date": f"30 Aug 2025 {i%24:02d}:{i%60:02d}:00",
                "prompt": f"Generated content {i}" * 100,  # Large prompt
                "metadata": {"size": 1024 * 1024}  # 1MB each
            })
        
        # Test memory-efficient processing
        processed_count = 0
        async for batch in manager._process_in_memory_efficient_batches(large_dataset, batch_size=100):
            processed_count += len(batch)
            # Verify batch size limit
            assert len(batch) <= 100
        
        assert processed_count == 10000

    @pytest.mark.asyncio
    async def test_concurrent_access_safety(self, enhanced_config):
        """üîÑ Test concurrent access and race condition handling"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Mock file operations
        async def mock_download(download_id):
            # Simulate download operation
            await asyncio.sleep(0.1)
            return f"download_{download_id}.mp4"
        
        # Test concurrent downloads
        tasks = []
        for i in range(10):
            tasks.append(mock_download(i))
        
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        # Verify all downloads completed
        assert len(results) == 10
        assert all("download_" in result for result in results)
        
        # Should complete concurrently (not sequentially)
        assert end_time - start_time < 2.0  # Much faster than sequential


class TestIntegrationFramework:
    """üîó Integration Test Framework"""

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_end_to_end_workflow(self, enhanced_config, mock_browser_with_optimizations):
        """üéØ Complete end-to-end workflow integration test"""
        page = mock_browser_with_optimizations
        manager = GenerationDownloadManager(enhanced_config)
        
        # Mock complete workflow components
        page.url = "https://example.com/gallery"
        page.query_selector_all = AsyncMock(return_value=[
            self._create_mock_container("30 Aug 2025 06:00:00", "New content 1"),
            self._create_mock_container("30 Aug 2025 05:45:00", "New content 2"),
        ])
        
        # Execute end-to-end workflow
        results = []
        async for result in manager.process_enhanced_downloads(page, max_downloads=2):
            results.append(result)
        
        # Verify workflow completion
        assert len(results) == 2
        assert all(result['success'] for result in results)
        assert all('generation_date' in result for result in results)
        assert all('prompt' in result for result in results)

    def _create_mock_container(self, date, prompt):
        """Helper to create mock container with metadata"""
        container = Mock()
        container.click = AsyncMock(return_value=True)
        
        # Mock metadata elements
        time_elem = Mock()
        time_elem.text_content = AsyncMock(return_value=date)
        
        prompt_elem = Mock()
        prompt_elem.text_content = AsyncMock(return_value=prompt)
        
        container.query_selector_all = AsyncMock(side_effect=lambda sel: 
            [time_elem] if 'time' in sel.lower() else [prompt_elem] if 'prompt' in sel.lower() else []
        )
        
        return container

    @pytest.mark.integration
    async def test_algorithm_component_interaction(self, enhanced_config):
        """‚öôÔ∏è Test interaction between algorithm components"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Test gallery navigator + boundary scroll manager interaction
        mock_page = Mock()
        navigator = RobustGalleryNavigator(mock_page)
        scroll_manager = BoundaryScrollManager(mock_page)
        
        # Verify component initialization
        assert navigator.page is mock_page
        assert scroll_manager.page is mock_page
        
        # Test component interaction
        mock_page.evaluate = AsyncMock(return_value=True)
        await navigator.initialize_gallery_state()
        await scroll_manager.scroll_to_boundary()
        
        # Verify both components interacted with page
        assert mock_page.evaluate.call_count >= 2


class TestPerformanceBenchmarking:
    """üìä Performance Benchmarking Tests"""

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_speed_optimization_benchmarks(self, enhanced_config):
        """‚ö° Benchmark speed optimizations (target: 40-55% improvement)"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Test adaptive timeout performance
        mock_page = Mock()
        mock_page.wait_for_load_state = AsyncMock(return_value=True)
        
        # Measure optimized timeout
        start_time = time.time()
        await manager._wait_with_adaptive_timeout(mock_page, "networkidle")
        optimized_time = time.time() - start_time
        
        # Compare with traditional fixed timeout simulation
        await asyncio.sleep(2.0)  # Simulate old fixed timeout
        traditional_time = 2.0
        
        # Verify improvement (should be significantly faster)
        improvement = (traditional_time - optimized_time) / traditional_time * 100
        assert improvement >= 40  # At least 40% improvement
        
    @pytest.mark.performance
    def test_memory_usage_benchmarks(self, enhanced_config):
        """üíæ Benchmark memory usage optimization"""
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss
        
        manager = GenerationDownloadManager(enhanced_config)
        
        # Process large dataset
        large_dataset = []
        for i in range(1000):
            large_dataset.append({
                "generation_date": f"30 Aug 2025 {i%24:02d}:{i%60:02d}:00",
                "prompt": f"Memory test prompt {i}",
                "metadata": {"index": i}
            })
        
        # Process with memory optimization
        processed = manager._process_metadata_efficiently(large_dataset)
        
        gc.collect()  # Force garbage collection
        final_memory = process.memory_info().rss
        
        # Memory growth should be minimal
        memory_growth_mb = (final_memory - initial_memory) / (1024 * 1024)
        assert memory_growth_mb < 50  # Less than 50MB growth
        
    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_large_scale_processing_performance(self, enhanced_config):
        """üèóÔ∏è Test performance with large-scale data processing"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Create large mock dataset (1000+ items)
        mock_containers = []
        for i in range(1000):
            container = Mock()
            container.click = AsyncMock(return_value=True)
            mock_containers.append(container)
        
        # Measure processing time
        start_time = time.time()
        
        # Process in optimized batches
        processed_count = 0
        async for batch in manager._process_containers_in_batches(mock_containers, batch_size=50):
            processed_count += len(batch)
            if processed_count >= 100:  # Test first 100 for speed
                break
                
        processing_time = time.time() - start_time
        
        # Should process efficiently (< 5 seconds for 100 items)
        assert processing_time < 5.0
        assert processed_count == 100


class TestRegressionSuite:
    """üîÑ Automated Regression Testing Setup"""

    @pytest.mark.regression
    def test_backward_compatibility(self, enhanced_config):
        """‚öôÔ∏è Test backward compatibility with existing configurations"""
        # Test old configuration format still works
        old_style_config = {
            "downloads_folder": "/old/path/downloads",
            "max_downloads": 25,
            "duplicate_mode": "finish"  # Old string format
        }
        
        # Should handle old format gracefully
        config = GenerationDownloadConfig()
        config.update_from_dict(old_style_config)
        
        assert config.downloads_folder.endswith("downloads")
        assert config.max_downloads == 25
        assert config.duplicate_mode == DuplicateMode.FINISH

    @pytest.mark.regression
    async def test_existing_functionality_preservation(self, enhanced_config):
        """üîí Ensure existing functionality still works after optimizations"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Test core functionality preserved
        assert hasattr(manager, 'process_downloads')
        assert hasattr(manager, 'extract_metadata')
        assert hasattr(manager, 'handle_duplicates')
        
        # Test file naming still works
        namer = EnhancedFileNamer(enhanced_config)
        filename = namer.generate_filename(
            Path("/test/video.mp4"), 
            creation_date="30 Aug 2025 05:15:30"
        )
        
        assert filename.startswith("vid_2025-08-30")
        assert filename.endswith(".mp4")

    @pytest.mark.regression
    def test_error_handling_regression(self, enhanced_config):
        """‚ö†Ô∏è Test error handling hasn't regressed"""
        manager = GenerationDownloadManager(enhanced_config)
        
        # Test invalid date handling
        namer = EnhancedFileNamer(enhanced_config)
        
        # Should handle invalid dates gracefully
        filename = namer.generate_filename(
            Path("/test/video.mp4"),
            creation_date="Invalid Date Format"
        )
        
        # Should use current timestamp format
        assert "vid_" in filename
        assert filename.endswith(".mp4")
        
        # Test malformed input handling
        result = manager._sanitize_metadata(None)
        assert result == ""
        
        result = manager._sanitize_metadata("")
        assert result == ""


# Performance test markers and configuration
@pytest.fixture(scope="session")
def performance_baseline():
    """Baseline performance metrics for comparison"""
    return {
        "traditional_download_time": 35.0,  # 35s baseline
        "traditional_memory_usage": 100,    # 100MB baseline
        "traditional_timeout": 10.0,        # 10s timeout
    }


# Integration with existing test infrastructure
class EnhancedTestRunner:
    """üéØ Enhanced test runner with comprehensive reporting"""
    
    @staticmethod
    def run_comprehensive_suite():
        """Run complete test suite with categorized reporting"""
        test_categories = {
            "algorithm": TestEnhancedAlgorithmSuite,
            "edge_cases": TestEdgeCasesAndErrorScenarios,  
            "integration": TestIntegrationFramework,
            "performance": TestPerformanceBenchmarking,
            "regression": TestRegressionSuite
        }
        
        results = {}
        for category, test_class in test_categories.items():
            print(f"üß™ Running {category} tests...")
            # Would integrate with pytest runner here
            results[category] = "PENDING"  # Placeholder
            
        return results


if __name__ == "__main__":
    # Example usage
    runner = EnhancedTestRunner()
    results = runner.run_comprehensive_suite()
    print("üéâ Test Suite Results:", results)